\section{Linear predictors}
This section is about linear predictors, the idea is to find an hyperplane 
to classify data.

\subsection{Structure}
Linear predictors are the simplest case of 
parametric algorithms.
We start with $X = \mathbb{R}^d$, so we handle only numerical
data. The predictors are defined as follows 
$$h : \mathbb{R}^d \rightarrow Y, \;\; h(x) = f(w^Tx), \;\; w \in \mathbb{R}^d$$
where $w$ is the description of the predictor, so it's what we are 
learning, and $f : \mathbb{R} \rightarrow \mathbb{R}$ is the activation function, 
and $w^Tx = \sum_{i=0}^d w_ix_i = ||w||||x|| \cos \theta$ where $\theta$
is the angle between the two vectors.

\paragraph{Binary classification}
In a binary classification task, $f = \mathit{sgn}$ 
\[
    \mathit{sgn}(z) = \begin{cases}
        +1 \mathit{\;if\;} x \in H^+\\
        -1 \mathit{\;if\;} x \in H^-
    \end{cases}\]
The halfspaces $H^+$ and $H^-$ are defined like this
$$H^+ = \big\{ x : w^Tx > c\big\} \;\;\mathit{and}\;\; H^- = \big\{ x : w^Tx \leq c\big\}$$

The predictor $h : \mathbb{R}^d \rightarrow \{-1, +1\}$ is defined 
as follows
$$h(x) = \mathit{sgn}(w^Tx - c)$$

The hyperplane $w$ has $d$ dimensions, and it is called homogeneous
if it crosses the origin. Basically if we learn $(w, c), c\neq 0$ we learned 
a non-homogeneous hyperplane, it holds that $v(w, -c), v\in \mathbb{R}^{d+1}$ is homogeneous.

Thus we can encapsulate the $c$ parameter into the hyperplane by adding 
dimension and an extra feature to the data points with value $1$.

\subsection{Training}
Note that $||w||$ is irrelevant to determine $\mathit{sgn}(w^Tx)
= \mathit{sgn}(||w||||x||\cos \theta)$, thus we can assume $||w|| = 1$.

Let $H_d = \{ w \in \mathbb{R}^d : ||w|| = 1\}$ we can rewrite the predictor 
output as follows 
$$h(x) = I\{ yw^Tx \leq 0\}$$
and we can apply the ERM over $H_D$, whose output is 
$$h_s = \min_{h \in H_d} \frac{1}{m} \sum_{t=1}^m I\{ y_tw^Tx_t \leq 0\}$$
this is nasty to minimize, as it is the sum of non-continuous functions, 
in fact the problem is in NP. Thus we can not say if a there exist an 
hyperplane with a bounded error.

\paragraph{Linearly separable data}
A training set is said to be Linearly separable it there exist a 
linear classifier with zero training error. 

In other terms, we can define a margin function $\gamma(u)$, $u \in \mathbb{R}^d$. Given a separating hyperplane, 
this quantity is greater than zero:
$$\gamma(u) = \min_{t=1, \dots, m} y_t u ^T x_t > 0$$
If we scale the margin like this $\gamma(u)/||u||$, we obtain the closest point to the separating hyperplane 
$u$.

In this case, we can efficiently find a separating hyperplane by using 
linear solvers. The idea is to express each correct classification 
the hyperplane makes as a system of linear equations 
$$y_tw^Tx_t > 0 \;\;\; t=1, \dots, m$$

\paragraph{Perceptron algorithm}
As typically linear solvers are quite complicated we use another 
way to find linear predictors.

The main idea is to start with an hyperplane and test the prediction on the 
data points, if an error is made, the parameters of the hyperplane are adjusted to make up for
the error. Basically, if $y_tw^Tx_t \leq 0$ then $w \gets w + y_tx_t$.

If $y_t = 1$, the algorithm moves $w$ towards $x_t$, in the opposite case 
it moves the plane away from the datapoint.

One might think that consequent updates may cancel off the progress 
that has been made so far but this is not the case, in fact a theorem tells 
that perceptron finds a solution if the dataset is linearly separable.
The algorithm terminates after a number of updates not bigger than 
$$\bigg( \min_{u: \gamma(u) \geq 1} ||u||^2\bigg) 
\bigg( \min_{t = 1, \dots, m} ||x_t||^2\bigg)$$

Note that although the algorithm terminates, there is no guarantee on 
the quality of the hyperplane found, in fact the returned hyperplane
strongly depends on the order of the data points passed to the perceptron, 
the goal would be to maximize the margin.

Also, the convergence may not be reached in polynomial time.

\subsection{Regression}

\paragraph{Linear regression}
In linear regression the predictors are linear functions
$$h : \mathbb{R}^d \rightarrow \mathbb{R}\;\;\; h(x) = w^Tx$$

Given a dataset the linear regression predictor is the ERM 
with respect to the square loss.
$$w_S = \min_{w \in \mathbb{R}^d} \sum_{t=1}^m (w^Tx_t - y_t)^2$$
The sum of square errors is a convex function. We can 
define $v = (w^Tx_1,\dots, w^Tx_m)$ and $y = (y_1, \dots, y_m)$ adn 
rewrite the previous equation as 
$$\sum_{t=1}^m (w^Tx_t - y_t)^2 = ||v - y||^2$$
If we consider all vectors as column vectors, $v = Sw$ where $S$ is a 
$m\times d$ matrix containing $x_1^T, \dots, x_m^T$, 
thus the previous equation becomes 
$$w_S = \min_{w \in \mathbb{R}^d} ||Sw - y||^2$$

As the function to minimize is a convex function we must find a zero 
gradient to minimize.