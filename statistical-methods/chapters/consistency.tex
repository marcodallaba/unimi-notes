\section{Consistency}

\paragraph{Consistent algorithm}
A learning algorithm is said to be consistent with respect
to some loss $l$, if, for all data distributions $D$ the following holds 
$$\lim_{m \rightarrow \infty} E[l_d(A(S_m))] = l_d(f^*)$$

Clearly, if $A$ is consistent, then the class of predictors that 
$A$ can output must be very large.
Also, $A$ must be \emph{nonparametric}.

\paragraph{Nonparametric algorithms}
A learning algorithm $A$ is nonparametric if the structure 
and consequently the memory footprint of the 
predictors output by $A$ depends on the training data.

An example of a nonparametric algorithm is \emph{k-NN}, 
as the Voronoi's cells generated by the algorithm depends 
only on training data.
Another example is found in decision trees, as the perpendicular 
boundaries learned strongly depends on training data. 

\begin{remark}
    Without a nonparametric algorithm it's impossible to output arbitrary 
    complex predictors, basically it's impossible to meet the consistency 
    requirement seen in the previous paragraph.
\end{remark}

\paragraph{K-NN consistency}
K-NN is not consistent fixed a $k$, but, if we assume the parameter 
to be function of the training set size $m$, we meet the consistency criterion.
$$k = K(m), \lim_{m\rightarrow \infty} K(m) = \infty$$

\paragraph{No free lunch theorem}
Let $a_1, a_2, \dots$ a sequence of positive numbers converging to zero
and such that $\frac{1}{16} \geq a_1 \geq a_2 \geq \dots$, 
$$\forall A\;\;\exists D \mathit{\;s. t.\;} l_D(f^*) = 0 \mathit{\;and\;} E[l_D(A(S_m))] \geq a_m
\forall m$$
where $A$ is a binary classifier with zero-one loss.

This means it's impossible to guarantee speed of convergence to Bayes risk for 
any data distribution.
There could be some distribution where the algorithm converges fast, 
but there exists others where it's really slow.

\begin{remark}
    We can beat the theorem by making assumptions on the data distribution.
\end{remark}

\paragraph{Convergence recap}
We can summarize the situation as follows:
\begin{enumerate}
    \item Under no conditions on $D$, there is no guaranteed convergence 
    rate to Bayes risk
    \item Under Lipschitz assumptions on $\eta$, the typical 
    nonparametric convergence rate to Bayes risk is $m^{-1/(d+1)}$, so exponentially slow in $d$, 
    this is the so called \emph{curse of dimensionality}
    \item Giving up consistency, and converging to $l_D(h^*)$
    where $h^*$ is the best of your class, e.g. ERM run on finite class $H$
    where $H$ is simple, then convergence rate is $\frac{1}{\sqrt{m}}$, 
    exponentially better than the nonparametric case
\end{enumerate}

\begin{remark}
    Should I use K-NN for a dataset with $10^3$ dimensions? Probably not, 
    for the curse of dimensionality.
\end{remark}