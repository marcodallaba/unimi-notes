\section{Probabilistic Information Retrieval}
The complexity of text analysis is such that we can't have 
a notion of correctness of an answer to a query, we therefore switch
idea and change \emph{true} to \emph{probable}.

\subsection{Review of probability theory}

\paragraph{Random variable}
A variable $A$ represents an events, so a subset of the space of possible
outcomes. Another representation of $A$ is through a random variable, 
that maps outcomes to real numbers.

\paragraph{Joint and conditional probabilities}
We denote $P(A|B)$ the probability of $A$ occurring given the occurrence of $B$.

\paragraph{Chain rule}
The following twos hold
$$P(A,B) = P(A \cap B) = P(A|B)P(B) = P(B|A)P(A)$$
$$P(\neg A, B) = P(\neg A|B)P(\neg A)$$

\paragraph{Partition rule}
If an event $B$ can be divided into an exhaustive set of disjoint sub-cases, 
the probability of $B$ is the sum of the sub-cases
$$P(B) = P(A,B) + P(\neg A, B)$$

\paragraph{Bayes rule} The following rule holds
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

This rule is a powerful tool to invert the conditional probability. 
The first step is to estimate the likelihood of the event $A$, thus $P(A)$, 
then we can derive a posterior probability $P(A|B)$ after having seen the 
event $B$, based on the likelihood of $B$ occurring when $A$ does or not hold.

The left member of the equation $P(A|B)$ is what we are trying to estimate, 
this could be for instance \emph{probability of a document to be relevant 
given that it contains a word X}.

$P(B|A)$ is on the other end the probability of observing the new evidence 
given the initial hypothesis. This could mean finding the \emph{probability 
of observing the word X given that a document is relevant}. 

$P(A)$ is called the prior. For instance \emph{probability of any document to 
be relevant}.

$P(B)$ is the marginal likelihood, for instance \emph{probability of observing 
the word X in any document}.

\paragraph{Odds of an event}
It is often useful to talk about the odds of an event 
$$O(A) = \frac{P(A)}{P(\neg A)} = \frac{P(A)}{1 - P(A)}$$



\subsection{Binary indipendence model}
\label{binary-ind}

In information retrieval we represent the relevant of a document 
given an information need as a random variable $R$, Given a document $d$
and a query $q$ we want to estimate the probability $$P(R = 1 |q, d)$$

Given a dictionary of terms $T = \left\langle t_1, \dots, t_n \right\rangle $ 
we model each document as a binary vector $\vec{x} = (x_1, \dots, x_n)$
where $x_i$ is 1 if the $i$th term is in the document.

The goal is to calculate 
$$O(R=1 | q, \vec{x}) = \frac{P(R=1|q, \vec{x})}{P(R=0|q, \vec{x})}$$

\paragraph{Simplifying the goal function}
Applying the Bayes rule leads to
\begin{equation}
    \begin{aligned}
        O(R=1 | q, \vec{x}) &= \frac{P(R=1|q, \vec{x})}{P(R=0|q, \vec{x})}
        = \frac{\frac{P(\vec{x}|R=1,q)P(R=1|q)}{P(q, \vec{x})}}{\frac{P(\vec{x}|R=0,q)P(R=0|q)}{P(q, \vec{x})}}\\
        &= \frac{P(\vec{x}| R = 1, q)}{P(\vec{x}| R = 0, q)}\cdot\frac{P(R=1|q)}{P(R=0|q)}
    \end{aligned}
\end{equation}

The left part of the result represent the odds of having that document 
given that it is relevant and given that query. On the right part we 
have the probability of the query producing relevant documents or not.

Assuming that the terms are independent, note that this is a strong assumptions, 
we can write
$$O(R=1| q, \vec{x}) = \frac{P(R=1|q)}{P(R=0|q)} \prod_{i=1}^{n} 
\frac{P(x_i|R=1, q)}{P(x_i|R=0, q)}$$ 
This product can be split according to the presence of terms in the document
$$O(R=1| q, \vec{x}) = \frac{P(R=1|q)}{P(R=0|q)} \prod_{i=1}^{n} 
\frac{P(x_i = 1|R=1, q)}{P(x_i = 1|R=0, q)} \prod_{i=1}^{n} 
\frac{P(x_i = 0|R=1, q)}{P(x_i = 0|R=0, q)}$$ 

To simplify the formula we introduce this values
$$p_i = P(x_i = 1|R=1, q), \;\; q_i = P(x_i = 1|R=0, q)$$
\begin{center}
    \begin{tabular}{c | c |c | c}
        & Document & $R=1$ & $R=0$\\
        \hline
        We observe the term & $x_i = 1$ & $p_i$ & $q_i$\\
        \hline
        We do not observe the term & $x_i = 0$ & $1- p_i$ & $1- q_i$
    \end{tabular}
\end{center}
Thus the previous equation becomes, where the first product iterates
over the words that appear in the document and the query, 
and the second product goes for words that do not appear in the document, 
but do on the queries. 
$$O(R=1| q, \vec{x}) = \frac{P(R=1|q)}{P(R=0|q)} \prod_{x=1, q=1} 
\frac{p_i}{q_i} \prod_{x=0, q=1} 
\frac{1- p_i}{1- q_i}$$ 
This assumes that $p_i = q_i$ for words that 
don't appear in a query, the assumption is motivated by the fact that a document
is relevant or not given a query, not by itself.

We now extend the right product to cover all the query terms, 
formally 
$$O(R=1| q, \vec{x}) = \frac{P(R=1|q)}{P(R=0|q)} \prod_{x=1, q=1} 
\frac{\frac{p_i}{q_i}}{\frac{1 - p_i}{1-q_i}} \prod_{q=1} 
\frac{1- p_i}{1- q_i}$$ 

We not observe that the quantity 
$\frac{P(R=1|q)}{P(R=0|q)}$ represents the odds of the query and does not 
depend on the document.
Also, the quantity $\prod_{q_i = 1} \frac{1- p_i}{1-q_i}$ depends on the terms 
that are in the query, it's so query dependent but it does not change 
for different documents.
This leads to a simplification of the previous formula, with some rewriting
$$O(R=1|q, \vec{x}) \approx \prod_{x_i=1, q_i = 1} 
\frac{p_i(1-q_i)}{q_i(1-p_i)}$$

\paragraph{Retrieval Status Value}
After the last simplification we can calculate the Retrieval Status Value
for a document given a query as the quantity
$$\mathit{RSV}_{q,d} = \log \prod_{x_i=1, q_i = 1} 
\frac{p_i(1-q_i)}{q_i(1-p_i)} = \sum_{x_i=1, q_i = 1} \log 
\frac{p_i(1-q_i)}{q_i(1-p_i)}$$

\paragraph{Estimating probabilities}
We now just need to estimate $p_i$ and $q_i$ and we are ready to go.
If a dataset has informations about relevant documents given a query, 
we can easily estimate them, given $R(t)$ and $N(t)$ occurrences of term $t$ in 
relevant and non relevant documents
$$p_i = \frac{R(t_i) + 0.5}{R + 1.0}, \;\; q_i = \frac{N(t_i) + 0.5}{N + 1.0}$$

Of course we don't always have the relevant and non relevant documents 
for queries, so we need some assumptions:
\begin{itemize}
    \item $p_i = q_i\;\mathit{if}\; t_i \notin q$, as discussed above
    \item $p_i = 0.5\;\mathit{if}\; t_i \in q$, we have the same probability of 
    observing or not a term in a randomly-picked relevant document
    \item $q_i \approx \frac{ |\{ d | t_i \in d\}| }{ |D| }$, 
    the set of non-relevant documents is approximated by the whole collection 
    of documents $D$
\end{itemize}
By applying the previous assumptions we obtain
\begin{equation}
    \begin{aligned}
        \mathit{RSV}_{q,d} &= \sum_{x_i=1, q_i = 1} \log \frac{p_i(1-q_i)}{q_i(1-p_i)}\\
        &= \sum_{x_i=1, q_i = 1} \log \frac{(1-q_i)}{q_i}\\
        &= \sum_{x_i=1, q_i = 1} \log \frac{N - N_i + 0.5}{N_i + 0.5}
    \end{aligned}
\end{equation}
By having $n$ and $N$ as the number of docs with $t$ and the total 
number of docs, respectively
$$\log \frac{(1-q_i)}{q_i} \approx \log \frac{N-n}{n} \approx \log \frac{N-n}{n} = \mathit{IDF}$$

This means that in practice, we are intersecting a document and a query terms, 
summing the \emph{IDF} of the words in the intersection, of course, by
considering the initial assumptions.

\subsection{Non binary models}
The non binary models takes into consideration the frequency of the terms 
and not simply a binary vector. They remove the binary assumption 
of the model presented in \vref{binary-ind}.

\paragraph{Okapi BM25}
In this non binary model, the RSV is estimated as follows
$$\mathit{RSV}_{q,v} = \sum_{t \in q}\log \frac{N}{\mathit{df}_t} 
\cdot \frac{(k+1)\mathit{tf}_{t,d}}{k((1-b) + b (\frac{L_d}{L_{\mathit{avg}}})) 
+ \mathit{tf}_{t,d}}$$
$k$ regulates the term frequency, while $b$ scales the importance of 
the document length.

\subsection{Probabilistic language modelling}
The goal is now to remove the independence assumption made in the model 
at \vref{binary-ind}.

In order to accomplish this, we need a model that can compute the probability 
of a sentence, i.e. a query, where the order of the words is important:
$$P(S) = P(w_1, \dots, w_n)$$
Another request is to have a generative model for sentences, so a model 
that estimates the probability of a word, given the previous terms in the 
sequence:
$$P(w_k | w_{k-1}, \dots, w_1)$$

The idea is to build a model on top of each document and calculate 
the probability $P(q|M_d)$ that a query $q$ has been generated by the model 
$M_d$.

\paragraph{Computing a sentence probability}
Given a sentence, for instance \emph{Mario plays a lot with Maria}, 
we need to compute the joint probability:
$$P(\mathit{Mario}, \mathit{plays}, \mathit{a}, \mathit{lot}, \mathit{with}, \mathit{Maria})$$
By applying the chain rule we can calculate the probability of the sequence
$$P(x_1, x_2, \dots, x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)\dots = \prod_i^nP(w_i|w_1, \dots, w_{i-1})$$ 

Calculating the quantity on real world documents could be impossible, 
as we might not be able to count long sequences of words to calculate the probability.
An approximation to the sequence probability is given by restricting the sequence length:
$$P(w_1, \dots, w_n) \approx \prod_i^nP(w_i | w_{i-k}, \dots, w_{i-1})$$
$k$ is a parameter, it should be 
small if the dataset is really small, or higher if there is enough data. For instance, 
if we take $k=2$ we are estimating the sequence by bi-grams
$$P(w_1, \dots, w_n) \approx \prod_i^nP(w_i | w_{i-1})$$
Note that to compute the probability of a bi-gram, we just count occurrences of a given bi-gram 
over all the possible ones.

\paragraph{Query likelihood model}
After building a model $M_d$ for each documents, following the idea presented on the previous paragraph, 
we can estimate relevance of documents given a query. 

So the probability that a query was generated by a document becomes:
$$P(q|M_d) = K_q\prod_iP(w_i|M_d)^{\mathit{tf}_{w_i, d}}\;\;\; 
K_q = \frac{\mathit{Len}(K_q)!}{\mathit{tf}_{t_1, q}! \dots \mathit{tf}_{t_N, q}!}$$
The $K_q$ is query dependent, so we can ignore it, it would take into consideration 
the query permutations.

We can also estimate the probability with this quantity 
$$P(q|M_d) \approx \prod_i\frac{\mathit{tf}_{t_i, d}}{L_d}$$

Note that $\mathit{tf}_{t_i, d}$ can be changed to the probability of the 
k-grams computed at the previous paragraph, it depends on how we choose to 
estimate the sentence probability.

\paragraph{Smoothing}
Note that all zero and non frequent term probabilities are a problem, 
for instance a long sequence could be interrupted by only one bi-gram not appearing in 
the document, the solution
is to smooth the numbers, for instance with the \emph{Laplace smoothing} that adds 
one to each bi-gram, the \emph{Bayesian smoothing} or the \emph{Linear interpolation} that 
takes into consideration the global document model.

\subsection{Other models}
\paragraph{Kullback-Leibler divergence model}
In this model the relevance term is computed as the quantity 
$$R(d, q) = \sum_i P(w_i|M_q)\log\frac{P(w_i|M_q)}{P(w_i|M_d)}$$
where $M_q$ is the model computed on the query.

\paragraph{Translation model}
The idea here is to use a generative model taking into consideration 
the possible translation of a query term.