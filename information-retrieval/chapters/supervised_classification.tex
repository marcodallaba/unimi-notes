\section{Supervised classification}

As discussed in section \vref{uns-class}, text classification is the 
problem of associating texts to classes, that are identified by labels.

\paragraph{Training data set}
A set of data examples are provided to the classifier to learn 
the mapping between data and labels. The goal is then to 
predict unlabeled data.

\paragraph{Unsupervised vs Supervised}
In the first case we don't have any information about 
classification criterions, while in the second we have classes 
associated to features, represented by the data points associated with 
labels.

\subsection{Models}

\paragraph{Decision trees}
The application of this model to text classification is rather 
easy. 
One could compute a vectorization of the text, in one of the many ways
seen and apply the tree learning algorithm.
This means imposing constraints on the presence and relevance 
of a certain word in a document.

One of the pros of having a decision tree is quick and easy visualization.
This means being able to see how the model computes a classification, 
to potentially avoid unethical decisions.

\paragraph{Rule-based classifiers}
These classifiers are based on rules, they are similar to 
decision trees, basically the antecedent of 
the rule is a subset of words and the consequent is a label.

\paragraph{Naive Bayes Classifiers}
The idea is that data has been generated by a mixture of $k$ components, 
where $k$ is the number of classes.
The goal is to estimate the probability 
\begin{equation}
    \begin{aligned}
        P(\mathit{class} \;|\;\mathit{features} ) &= 
        \frac{P(\mathit{class}) 
        P(\mathit{features}\;|\;\mathit{class})}{P(\mathit{features})}
        \\&= P(\mathit{class}) 
        P(\mathit{features}\;|\;\mathit{class})
    \end{aligned}
\end{equation}
where the probability of a class is the size of the class over the size 
of the corpus.
Also, the probability of features given the class, is simply 
the product of the count of a given words over the total count of the words in 
the class.

\paragraph{K-NN classifiers}
The application of the model to text is straight forward after 
text vectorization.

\paragraph{Linear classifiers}
The idea is to learn an hyperplane separating data, again, this is 
not different to any linear classifier task.

The main idea is to minimize a loss function plus 
a regularizer term.

\paragraph{Support Vector Machines}
A linear classifier is the Support Vector Machine, 
the goal is to find the optimal hyperplane separating data, 
i.e. the one with greater margin between the two classes.

To accommodate non linearly separable data one could use the kernel 
trick. Basically we perform a transformation of the dataset 
to map it in a high dimensional space where data point could be 
linearly separable.  There are many possible kernels.

\subsection{Classification strategies}
When the system is efficient in performing binary classification
we can apply it to multi-class classification by transforming the 
problem in a binary task.

\paragraph{One vs Rest}
The goal here is to learn a binary classifier between a class and all 
the others.

\paragraph{One vs One}
The idea here is to train a classifier between every pair of classes.

\subsection{Multi-label classification}
To assign multiple labels to data point we need to exploit 
something in the previous classifiers. 

For instance, we could use the distance of the many one vs one classifiers 
as confidence measures for labels. 

Another way is to take the power set of classes and train many classifiers 
as before. We are left with a multi-class classification 
where each class is a combination of labels.

\subsection{Hyperparameters tuning  and evaluation}

\paragraph{Model selection}
The activity of tuning hyperparameters to find the 
best model instance to solve a task.

\paragraph{Cross Validation}
The idea is to split the dataset in $k$ folds and consider 
$k-1$ folds as training and the last one as test set.
We iterate over the possible training sets and average the error.

\paragraph{Confusion matrix}
We can arrange the errors in a confusion matrix to see 
how many elements of class $i$ are classified as class $j$. 
Obviously the goal is to have positive values only on the 
diagonal.
