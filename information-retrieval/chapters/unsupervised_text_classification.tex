\section{Unsupervised text classification}

\paragraph{Text classification}
The problem of associating texts, so documents, to classes, represented by labels.
Classes may be a partition of the document space, or they can overlap.
The approaches to solve the problem are basically two, pre-trained models, 
so supervised, or based only on documents, so unsupervised.

Depending on the number of classes and how the cover the document space, we define 
four tasks
\begin{center}
    \begin{tabular}{c | c |c}
            & 2 classes & Many classes\\
            \hline
            Disjoint classes & Binary classification & Multi-class classification\\
            \hline
            Overlapping classes & Soft binary classification & Multi-label classification\\
    \end{tabular}
\end{center}

Note that ranking can be seen as soft binary classification.

\subsection{Clustering}

\paragraph{Goal}
Clustering is the problem of grouping objects in clusters, in particular
given a distance metric, 
we want to minimize the distance within a cluster and maximize the one
between different clusters.

\paragraph{Parameters}
Choosing the right number of clusters is pretty difficult, a high number 
leads to consistency inside clusters, but poor aggregation, while a low number 
increases aggregation but reduce quality of the single clusters.

\paragraph{Types of clustering}
There are many approaches to clustering, one could for instance 
consider flat or hierarchical clustering, in the first one there is no structure, where 
in the second clusters might contain other clusters. 
There is also a difference in cluster assignment, one could assign in an hard or soft way.

\subsubsection{Support to document retrieval}

\paragraph{Terminological analysis}
After computing a clustering of documents we could analyze terminology 
of documents of the same cluster to extract relevant terms, this 
information can then be used for query expansion.

\paragraph{Cluster pruning}
By selecting representative documents for clusters, we can compare a query 
only to those documents. After finding a match all the documents in the cluster 
are returned.

\subsubsection{Evaluation}
Given $\Omega = \{ \omega_1, \dots, \omega_n\}$ the set of clusters 
and $\mathbb{C} = \{ c_1, \dots, c_n\}$ the set of classes, we can evaluate 
the clustering by focusing on pairs of documents.

Let's consider the pair $(x_i, x_j)$, we ask ourselves if 
$$\exists \omega_k : (x_i, x_j) \in \omega_k\;\;\exists c_z : (x_i, x_j) \in c_z$$
They represent the clustering and the real world scenario.
If the two are true, we find a true positive, if they are both false a true negative.
In the case the first one is true and the second is not, we have a false positive, 
a false negative otherwise.

We can observe that choosing a small number of clusters leads to a large 
quantity of false positives, while a fine grain partition, i.e. a high number 
of clusters, produces a high number og false negatives.

\paragraph{Measures}
We can use some measures of quality in clustering, 
for instance 
$$\mathit{Rand} = \frac{\mathit{TP} + \mathit{TN}}{\mathit{TP} + \mathit{TN} + \mathit{FP} + \mathit{FN}}$$
but also \emph{Purity} and \emph{Normalized Mutual Information}.

\subsubsection{K-means}
This approach starts with $k$ random centroids and assign each data point 
to the closets one. We then recompute the centroids with the current clusters 
and repeat the first point until termination.

The termination criterion could be a fixed number of iterations, no change 
in documents assignments or \emph{RSS} measure below a threshold:
$$\mathit{RSS} = \sum_{k=1}^K\sum_{\vec{x} \in \omega_k}|\vec{x} - \vec{\mu}(\omega_k)|^2$$
where the measure is the sum of the distances between a point and it's centroid squared.
Note that at each iteration the \emph{RSS} decrease.

Two issues of this approach is that the initial position of clusters
changes the results a lot, and also, the $k$ parameter is crucial to the 
algorithm.
A good value for $k$ is $K = \min_K(\min(\mathit{RSS}_K) + \lambda K )$

\subsection{Model-based clustering}
A k-means generalization is obtained by interpreting the centroids as a model 
that generates data. A centroid with some added noise can generate a document.

\paragraph{Idea}
Instead of generating classes, we start from the points, 
we assume that they were generated with a generative model 
and we estimate the latent model parameters.

\paragraph{Latent parameters}
$\Theta = \{ \vec{\mu}_1, \dots, \vec{\mu}_n\}$ are the centroids 
to be found by k-means.

$L(D | \Theta)$ is the log-likelihood that the data $D$ wa generated by $\Theta$, 
this is the objective function, so $\Theta$ becomes:
$$\Theta = \max_{\Theta} L(D|\Theta) = \max_\Theta\sum_{n=1}^N \log P(d_n | \Theta)$$

That means maximizing the sum of probabilities that documents are generated by 
the model. We are in a soft clustering scenario, as we are 
basically dealing with probabilities
as cluster assignments.

\subsubsection{Expectation Maximization algorithm}
This is an iterative algorithm that maximizes $L(D|\Theta)$, but can also be 
used to find latent models in a variety of applications.

Let's consider a multivariate Bernoulli distribution for data, 
so all documents are binary vectors, 
we want to estimate the probability that a given document is assigned to a
specific cluster given the models parameters.
$$P(d | w_k; \Theta) = \prod_{t_m \in d}q_{mk} 
\cdot \prod_{t_m \notin d}(1-q_{mk})\;\;\;\Theta_k = (\alpha_k, q_{1k}, \dots, q_{Mk})$$
where $q_{mk} = P(U_m =1 | \omega _k)$ is the probability that a document from the cluster 
$\omega_k$ contains the term $t_m$.

$\alpha_k$ si the prior of the cluster $k$, so the probability that of a document 
to be in that cluster, not having any information about it.

\paragraph{Process}
We start by generating a document by picking a cluster $w_k$ with probability 
$\alpha_k$, generating terms with probability $q_{mk}$. 

We need to estimate the two parameters, and we do that iteratively similarly to k-means.
The iteration is composed by two steps, \emph{Maximization} and \emph{Expectation}.

\paragraph{Maximization step}
The goal here is to estimate the parameters of the model, so the prior and the 
terms probability given the cluster:
$$q_{mk} = \frac{\sum_{n=1}^N r_{nk}I(t_m \in d_m)}{\sum_{n=1}^N r_{nk}}
\;\;\; \alpha_k = \frac{\sum_{n=1}^N r_{nk}}{N}$$
where $I$ can be one or zero wether the term appears or not in the 
document, and $r_{nk}$ is the soft assignment of the document $n$ to the 
cluster $k$.


\paragraph{Expectation step}
We now estimate the probability of each term to be assigned to a cluster.
$$r_{nk} = \frac{\alpha_k(\prod_{t_m \in d_n}q_{mk})(\prod_{t_m \notin d_n}(1-q_{mk}))}
{\sum_{k=1}^K\alpha_k(\prod_{t_m \in d_n}q_{mk})(\prod_{t_m \notin d_n}(1-q_{mk}))}$$
this is basically, for each cluster, the prior, multiplied by the product of $q_{mk}$ or $1- q_{mk}$ for each 
term in the document
computed before, 
normalized by the same quantity over all the clusters.

\paragraph{Why do EM works?}
The main idea of the algorithm is to push documents that contains the same 
words in the same cluster. Basically if two words appears in a document 
they should have a high probability of being assigned to the same cluster, 
so they have the same $q_{mk}$. This leads to assigning documents with the 
same words in the same clusters.
