\section{Topic modeling}

\paragraph{Vector space models problems}
The approaches seen so far did not take into 
consideration semantics of words. 
We understood that the vector space models suffer 
from high dimensionality, the terms also could be synonyms, 
leading to ambiguity, in fact two words that means 
the same thing are two different dimensions in the model.
\subsection{Linear algebra recap}

\paragraph{Eigenvalues and eigenvectors}
Given a square matrix of size $n$, if we find a
 $n$ dimensional vector such that
$$A\vec{x} = \lambda\vec{x}$$
$\lambda$ is a eigenvalue and $\vec{x}$ is an eigenvector.
It also holds that 
$$(A - \lambda I )\vec{x} = 0 \implies |A - \lambda I| = 0$$
where $I$ is the identity matrix, and the implication holds as $\vec{x}$ is non-zero.
In general the second part of the implication has $n$ solutions, so 
$n$ eigenvalues associated with a certain eigenvector.

After finding the solutions we can write that 
\begin{equation}
    \begin{aligned}
        AX &= A[\vec{x_1}, \dots, \vec{x_n}] = A\vec{x_1} + \dots + A\vec{x_n}\\
        & = \lambda_1\vec{x_1} + \dots + \lambda_n\vec{x_n} = \varLambda X  
    \end{aligned}
\end{equation}
Where $\varLambda$ is the diagonal matrix of the eigenvalues, and $X$ the matrix 
with eigenvectors. 

\paragraph{Diagonalization}
If the $n$ eigenvector found at the previous step are independent, 
and the matrix $A$ is invertible, which is not a problem as its square, 
we can write 
$$AX = X\varLambda \rightarrow AXX^{-1} = XAX^{-1} \rightarrow A = X\varLambda X^{-1}$$

This representation is called diagonalization of $A$.
Also, if $A$ is symmetric, we can write
$$A = X\varLambda X^{\top}$$

\subsection{Latent Semantic Indexing}
The goal is to discover topics that motivate data
by using matrix factorization techniques, while 
taking in consideration the possibility of expressing
the same topic with different words.

\paragraph{Diagonalization}
We learned that if a matrix is symmetric we can write the 
diagonalization as $A = X\varLambda X^{\top}$.
The question is, what happens if we multiply the 
document-term matrix by itself? 

We find a symmetric matrix that somehow encapsulates a notion of similarity 
of documents because if two documents share the same words the 
product will be really high for that two documents.

If we multiply the term-document matrix we obtain a similar result.

\paragraph{Singular Value decomposition}
The idea behind the approach is to define the term document matrix 
$A$ in terms of $$A = U \Sigma V^\top$$
this means finding something that holds together documents and terms. 
The terms in the diagonalization represent:
\begin{itemize}
    \item$V$ is the matrix with the eigenvectors of $AA^\top$
    \item$U$ is the matrix with the eigenvectors of $A^\top A$
    \item $\Sigma$ is the diagonal matrix composed starting from the eigenvalues
    of $A A^\top$ and $A^\top A$, which are the same
\end{itemize}
Note that the eigenvalues in $\Sigma$ are sorted on the diagonal.

\paragraph{Defining topics}
After applying singular value decomposition 
to the document term matrix we have the latent topic matrix
$\Sigma$.
We can consider a new matrix $\Sigma_{k \times k}$, that 
somehow encapsulate the top $k$ relevant topics for 
the documents.
Note that after this cut of $\Sigma$ we get a diagonalization 
that is an approximation of the original matrix.
$$A \approx U \Sigma_{k \times k} V^\top$$

To make topics human understandable 
we assign each a set of words, and also, we can assign to each 
document a topic, in a soft clustering fashion.

Basically, given the three matrixes that SVD outputs, after 
cutting $\Sigma$ as discussed above,   
we can discover what topics have the documents by multiplying 
$U$ and $\Sigma_{k \times k}$. The output will be a matrix with document 
on rows and topics on columns, where the absolute value of 
a cell represent how strong a topics is present in a document.

In the same way we can find a relation between topics and words 
by multiplying $\Sigma_{k \times k}$ and $V^\top$.

\subsection{Latent Dirichlet Allocation}
This approach relies on probability theory to find topics
in documents. 
In the LSI approach we tried to find a \emph{glue}, formally $\Sigma$,
between document and terms.

In LDA the goal is to estimate the probabilistic relation 
between topics and documents, basically the probability that 
a given document contains a given topic, and the one that 
a topics contains a certain word.

\paragraph{Elements}
The basic elements of the approach are:
\begin{itemize}
    \item $\phi^{(k)}$ probability distribution over the vocabulary 
    for the kth topic
    \item $\theta_d$ document distribution over topics
    \item $z_i$ topic index for the word $w_i$
    \item $\alpha, \beta$ hyperparameters that govern the first 
    two distributions
\end{itemize}

\paragraph{Estimating the generative model}
To find the topics and document distributions we 
assume that they were generated by a given distribution
and try to estimate them.
