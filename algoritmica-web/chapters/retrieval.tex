\section{Retrieval}
L'attenzione passa ora alle pagine scaricate. 
Ogni documento è numerato e contiene una certa quantità di caratteri, l'obiettivo 
ora è indicizzarli.

\begin{remark}
    Tipicamente bisogna fare guessing per capire la codifica del testo, esistono 
    tabelle dedicate a questo in ogni browser.
\end{remark}

\paragraph{Segmentazione dei documenti}
Partendo da un documento di ottengono sequenze di token, in qualche modo, per esempio 
spezzando il testo agli spazi, rimuovendo le stop-words etc.
Tipicamente si applicano operazioni di normalizzazione, come troncamento, lemmatizzazione, etc.

\paragraph{Matrice termini-documenti}
Ordinando la totalità dei termini nei documenti si può ottenere una rappresentazione
matriciale con documenti sulle colonne e termini per righe. Una cella indica 
la presenza o meno di una certa parola nel testo di un documento.

Quello che si fa poi è creare la trasposta ordinata, ovvero una matrice che 
per ogni token ha una lista di interi che rappresentano i documenti in cui esso 
è contenuto.

\subsection{Codici istantanei}

Un codice è un sottoinsieme di parole binarie: $C \subseteq 2^* = \{0,1\}^*$.

\begin{definition}
    Si dice che $x$ è un prefisso di $y$, $x \preceq y$ se $\exists z \in 2^*\;t.c.\;xz = y$, ovvero se concatenato ad un altra parola binaria ottengo $y$.    
\end{definition}

\begin{definition}
    Due parole $x$ e $y$ sono confrontabili se $x \preceq y$ oppure $y \preceq x$.
\end{definition}

\begin{definition}
    Un codice $C$ si dice istantaneo se $\forall x,y \in C$, $x$ e $y$ sono inconfrontabili, ovvero privo di prefissi.
\end{definition}

\begin{definition}
    Un codice $C$ si dice completo se ogni parola binaria è confrontabile 
    con una parola $x \in C$. Ovvero, aggiungendo una qualsiasi parola binaria, 
    il codice non è più istantaneo.
\end{definition}

\paragraph{Motivazioni}
Memorizzare sequenze di interi ordinate in maniera crescente può essere fatto in maniera
efficiente memorizzando gli scarti, differenze, tra due interi consecutivi.
$$1, 6, 7, 10, 12 \longrightarrow 1, 5, 1, 3, 2$$
Queste liste potrebbero essere le righe della matrice termini-documenti.

Un'altra proprietà interessante di un codice istantaneo è l'univocità di decodifica 
scorrendo una sequenza di parole concatenate. 
Infatti, visto che nessuna parola è prefisso di un'altra, ho un solo modo di decodificare una 
sequenza di zeri e uni.

\paragraph{Ordinamento nei codici istantanei}
Due esempi di codici istantanei sono:
\begin{itemize}
    \item $k = 0^k1$
    \item $k = 1^k0$
\end{itemize}
Il secondo codice mantiene l'ordine, il secondo no.

\paragraph{Importanza della completezza}
Se un codice è istantaneo e completo si può decodificare ogni stringa binaria 
casuale in modo univoco, altrimenti no.

\subsubsection{Disuguaglianza di Kraft-Mcmillan}

\begin{definition}
    Sia $C$ un codice istantaneo, vale: 
    $$\sum_{w \in C} \frac{1}{2^{|w|}} \leq 1$$
\end{definition}
Se $C$ istantaneo, $C$ è anche completo sse $\sum_{w \in C} \frac{1}{2^{|w|}} = 1$.
Nell'interpretazione geometrica\footnote{Vedi dimostrazione} non sarebbe
libero nessun intervallo. 

\begin{remark}
    Il fatto che la sommatoria dia un valore piccolo non dice nulla su un linguaggio 
    qualsiasi, non è vera quindi l'implicazione inversa.
\end{remark}

\paragraph{Teorema interi}
Siano $t_0 \leq t_1 \leq \dots \leq t_n$ interi, se vale:
$$\sum_{i \in N} \frac{1}{2^{t_i}} \leq 1$$
Allora esiste un codice istantaneo con $n$, dove la parola i-esima ha lunghezza $t_i$.

Questo fatto rende possibile un'eventuale correzione, infatti, se avessi un codice con quelle lunghezze 
ma non fosse istantaneo, so che potrei sistemarlo.

\paragraph{Visione probabilistica}
La quantità $\frac{1}{2^{|w|}}$ si può vedere come una probabilità, visto che nel caso 
in cui un codice sia istantaneo tutte quelle quantità si sommano ad uno.

Potrei pensare di codificare probabilità maggiori a parole più corte. 
La disuguaglianza di prima, e l'algoritmo greedy associato\footnote{Non presentato qui, ma simile a Huffman}
genera il codice ottimo per una distribuzione di probabilità del tipo $\frac{1}{2}, \frac{1}{4}, \dots, \frac{1}{2^k}$.

\subsection{Esempi di codici}

\subsubsection{Codici binari}

\paragraph{Notazione binaria ridotta}
Codice binario in cui si rimuove il primo 1. Per esempio: $$1001010 \rightarrow 001010$$

\paragraph{Codice binario minimale} 
Sia $s = \lceil \log k \rceil$, se $x < 2^s - k$, esso è codificato dall'$x$-esima parola binaria
di lunghezza $s-1$, altrimenti tramite la $(x-k + 2^s)$-esima parola binaria di lunghezza $s$.



\subsubsection{Elias}

\paragraph{Codice $\gamma$}
Ogni numero è scritto in notazione binaria ridotta, ovvero in binario senza il primo uno, preceduto dalla sua rappresentazione unaria.
\begin{center}
    \begin{tabular}{|l | r | r |}
        \hline
        Numero & Codifica $\gamma$ & Unario\\
        \hline
        1 & 1 & 1\\
        2 & 010 & 01\\
        3 & 011 & 001\\ 
        4 & 00100 & 0001\\ 
        5 & 00101 & 00001\\ 
        6 & 00110 & 000001\\ 
        \dots & \dots & \dots\\
        \hline
    \end{tabular}
\end{center}

La lunghezza di $x$ nella codifica $\gamma$ è $2\lambda(x) + 1$, dove $\lambda$ è
$\lfloor \log_2 x\rfloor$.
In binario invece spenderei $\lambda(x)$, ma non è istantaneo. In unario invece 
spendo una quantità pari a $x$, che in casi in cui i numeri siano piccoli va bene, altrimenti no.

\paragraph{Codice $\delta$}
Molto simile al precedente, invece che usare l'unario, 
scrivo davanti alla rappresentazione binaria ridotta la lunghezza 
del codice $\gamma$ associato all'x-esimo numero.

Confrontando i codici visti fin'ora: 
\begin{center}
    \begin{tabular}{|l | l |}
        \hline
        Codifica & Lunghezza\\
        \hline
        Unario & $x+1 = O(x)$\\
        Elias $\gamma$ & $2\lambda (x+1) + 1 = 2\log (x+1) + 1$\\
        Elias $\delta$ & $x$\\ 
        \hline
    \end{tabular}
\end{center}

Il codice $\delta$ sui dice asintoticamente ottimo, visto che utilizza 
un numero logaritmico di bit più qualcosa di più piccolo. 
Nel lungo termine risparmia rispetto agli altri due codici.

\subsubsection{Golomb}

Si fissa un parametro $b$ che si dice modulo del codice. Preso un 
$x > 0$ si codifica in unario la quantità $\lfloor x/ b \rfloor$, 
seguito dalla binaria minimale di $x\;\textit{mod}\;b$.

Dato un codice, posso tornare al numero con $b\lfloor x/ b \rfloor + x\;\textit{mod}\;b$.

Fissato $b = 3$, il codice diventa 

\begin{center}
    \begin{tabular}{|l | r | r |}
        \hline
        Numero & Codifica Golomb \\
        \hline
        0 & 10\\
        1 & 110\\
        2 & 111\\ 
        3 & 010\\ 
        4 & 0110\\ 
        5 & 0111\\ 
        \dots & \dots\\
        \hline
    \end{tabular}
\end{center}

\begin{remark}
    Golomb con $b = 1$ la prima parte del codice equivale all'unario. Inoltre, questo codice ha problemi simili all'unario, 
    la prima parte infatti cresce asintoticamente come $x$, 
    nella pratica è minore, ma comunque occupa molto spazio. 
\end{remark}

\subsubsection{Codici a blocchi a lunghezza variabile}
Quello che si fa è memorizzare tanti blocchi per un singolo 
numero. Il primo bit di ogni byte contiene il fatto che 
il codice è terminato oppure no. Nel caso in cui non sia terminato 
bisogna leggere uno o più altri byte per completarlo. 

Un miglioramento è mettere tutti i bit \emph{di continuazione} 
nel primo byte, per capire fin da subito quanto proseguire.

Sio può pensare dal punto di vista logico ad una lista concatenata 
che contiene pezzi di un numero, la decodifica consiste nello scorrere 
la lista e concatenare i valori in ogni nodo.

\subsubsection{Distribuzione intesa}
Ogni codice crea implicitamente una distribuzione intesa, 
ovvero parole più corte hanno probabilità maggiore, quindi, si 
può studiare come varia questa probabilità in relazione alla lunghezza 
che assumono le parole nei vari codici.\footnote{Vedi appunti, distribuzioni intese.}

Conoscendo la distribuzione di probabilità dei dati si può utilizzare 

\subsubsection{PFOR-DELTA}
Compressione che funziona molto bene quando numeri piccoli compaiono 
molto più frequentemente di quelli grandi.

Si fissa una dimensione di blocco $B \approx 256$, la compressione avviene 
per blocchi di $B$ elementi alla volta.
Quello che si fa è scorrere gli elementi di un blocco e scegliere 
un numero di bit $b$ che rappresenti almeno una percentuale $\alpha \approx 90\%$ della totalità dei numeri del blocco. 

\paragraph{Compressione}
A questo punto scorro la lista, 
se un numero è rappresentabile in $b$ bit lo scrivo, altrimenti 
scrivo un codice di escape, per esempio tutti uni o zeri.\footnote{Necessito quindi di un valore libero per questo codice nei $b$ bit.} Accodo alla fine i numeri non rappresentabili, tutti a lunghezza $a$,
la minima per rappresentarli tutti.
All'inizio della lista inserisco il numero $b$ e il valore $a$.

\paragraph{Decompressione}
La decompressione della lista è lineare, bastano due puntatori, uno all'inizio e uno che parte 
prima dei numeri accodati.
Si ottiene una buona compressione, $b$ viene molto piccolo se tanti valori sono piccoli, inoltre nell'$alpha$-percento dei casi 
non devo accedere alla memoria accodata alla lista, che è la parte più 
costosa della decompressione.

