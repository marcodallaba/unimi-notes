\subsection{Tecniche di pricing}
L'idea è quella di attribuire ad ogni elemento da inserire in 
una soluzione un costo. I costi permettono di scegliere gli elementi 
più vantaggiosi e di analizzare il tasso di approssimazione di questi algoritmi.

\subsubsection{Minimum Set Cover}
Nel problema di Set Cover l'obiettivo è quello di coprire l'universo $U$, 
utilizzando seubset di esso che hanno un costo. Si vuole ottenere il costo
minimo, dato come somma dei costi dei set che si sono scelti.

Formalmente: \\
\emph{Input}: $s_1, \dots, s_m$, $\bigcup_{i=1}^m s_i = U$, $|U| = n$\\
\emph{Output}: $C = \{s_1, \dots, s_n\}$, tali che, $\bigcup_{s_i \in C} s_i = U$\\
\emph{Costo}: $w = \sum_{s_i \in C} w_i$\\
\emph{Tipo}: min\\

\paragraph{Funzione armonica}
La funzione armonica è definita come: 
\begin{equation}
    \begin{aligned}
        H: \mathbb{N}^{>0} \rightarrow \mathbb{R}\\
        H(n) = \sum_{i = 1}^{n} \frac{1}{i}  
    \end{aligned}
\end{equation}
Vale la seguente proprietà: 
% \begin{equation}
%     \begin{aligned}
%         H(n) \leq 1 + \int_{1}^{n} \frac{1}{x} \,dx \leq 1 +
%         \big[ \ln n\big]_1^n = 1 + \ln n\\
%         \ln (n+1) \leq H(n) \leq 1 +\ln n
%     \end{aligned}
% \end{equation}
$$\ln (n+1) \leq H(n) \leq 1 +\ln n$$

\paragraph{Greedy Set Cover}
L'algoritmo effettua ad ogni iterazione una scelta greedy, 
si sceglie l'insieme che minimizza il rapporto tra prezzo e copertura 
dell'universo.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwIn{$s_1, \dots, s_m$, $w_0, \dots, w_n$}
    \KwResult{Scleta di sottoinsiemi che copre l'universo}
     $R \gets U$\\
     $C \gets \emptyset$\\
     \While{$R \neq \emptyset$}{
         $S_i \gets \min(s_1, \dots, s_m, \frac{w_i}{|S\cap R|})$\\
         $C.add(S_i)$\\
         $R \gets R \setminus S_i$

     }
     \Return{$C$}
     \caption{GreedySetCover}
\end{algorithm}

\begin{remark}
    \label{oss1set}
    Il costo della soluzione equivale a $$w = \sum_{s\in U}c_s$$ ovvero la somma dei costi 
    degli insiemi scelti.
\end{remark}
\begin{remark}
    \label{oss2set}
    Per ogni $k$, il costo degli elementi in $s_k$, ottengo 
    $$\sum_{s \in S_k}C_s \leq H(|S_k|) \cdot  W_k$$
\end{remark}
\begin{proof}
    Sia $S_k = \{s_1, \dots, s_d\}$ un insieme tra quelli da scegliere, e siano 
    i suoi elementi elencati in ordine di copertura\footnote{Per chiarezza, l'insieme $S_k$ non verrà scelto, 
    ma i suoi elementi saranno coperti da altri insiemi che intersecano con esso.}.

    Consideriamo ora l'istante in cui si copre $S_h$ tramite un quale insieme $S_h$.
    Si può notare che, visto che gli elementi sono in ordine di copertura: 
    $$R \supseteq \{S_j, \dots, S_d \}$$
    Inoltre, visto che gli elementi di $S_k$ sono in ordine di copertura: 
    $$|S_k \cap R| \geq d-j+1$$
    Riguardo al costo dell'elemento $j$, e in geenerale per tutti i $j$, vale:
    \begin{equation}
        \begin{aligned}
            C_{s_j} = \frac{W_h}{|S_h \cap R|} \leq \frac{W_k}{|S_k \cap R|} && \text{\emph{h} minimizza quel rapporto}\\
            \leq \frac{W_k}{d-j+1} && \text{equazione precedente}\\
        \end{aligned}
    \end{equation}
    Considerando ora tutti gli elementi di $S_k$: 
    \begin{equation}
        \begin{aligned}
            \sum_{s\in S_k} C_s \leq \sum_{j=1}^{d}\frac{W_k}{d-j+1} = \frac{W_k}{d} + \frac{W_k}{d-1} \dots && \text{La relazione vale per tutti i j}\\
            = W_k(1 + \frac{1}{2}, \dots, \frac{1}{d}) = H(d)W_k = H(|S_k|)W_k && \text{Sviluppo e raccolgo, ottengo l'oss.}
        \end{aligned}
    \end{equation}
\end{proof}
\begin{theorem}
    Greedy Set Cover fornisce una $H(M)$-approssimazione per Set Cover, dove $M=\max_i|S_i|$
\end{theorem}
\begin{proof}
    Sia il peso della soluzione ottima $$w^* = \sum_{S_i \in C^*} w_i$$
    Per l'osservazione \ref{oss2set} vale che: 
    $$w_i \geq \frac{\sum_{s \in S_i}C_s}{H(|S_i|)} \geq \frac{\sum_{s \in S_i}C_s}{H(M)}$$
    Visto che gli $s_i \in C^*$ sono una copertura, per l'osservazione \ref{oss1set}:
    $$\sum_{S_i \in C^*}\sum_{s \in S_i}C_s \geq \sum_{s \in U} C_s = w$$
    Inoltre, vale che, sfruttando le due disequazioni appena scritte: 
    \begin{equation}
        \begin{aligned}
            w^* = \sum_{S_i \in C^*} w_i \geq \sum_{S_i \in C^*}\frac{\sum_{s \in S_i}C_s}{H(M)} \geq \frac{w}{H(M)}\\
            \implies \frac{w}{w^*} = H(M)
        \end{aligned}
    \end{equation}
\end{proof}
\begin{corollary}
    Greedy Set Cover fornisce una $O(\log n)$-approssimazione, non quindi costante 
    come gli algoritmi precedenti.
\end{corollary}
\begin{remark}
    L'analisi è tight, non esiste un algoritmo migliore, quindi
    Greedy Set Cover $\notin$ APX, bensì, $\in \log(n)$-APX, una classe in cui 
    si accetta un'approssimazione che peggiora logaritmicamente nell'input.
    Esistono varie $f$-APX.
\end{remark}
\begin{proof}
    Per dimostrare la tightness, ecco un esempio in cui Greedy Set Cover va male.

    Consideriamo l'insieme $S$ di set tra cui scegliere così formato:
    \begin{enumerate}
        \item Due insiemi grandi $\frac{n}{2}$ che uniti coprono tutti gli elementi, 
        di costo $1+\epsilon$
        \item Un insieme che copre $\frac{n}{4}$ elementi degli insiemi 1 e 2 al punto 1, di costo 1
        \item Un insieme che copre $\frac{n}{8}$ elementi degli insiemi 1 e 2 al punto 1, di costo 1
        \item Un insieme che copre $\frac{n}{16}$ elementi degli insiemi 1 e 2 al punto 1, di costo 1\\
        \dots
    \end{enumerate}
    Al primo passo, si sceglie l'insieme del punto 2, visto che
    il suo costo equivale a $\frac{1}{\frac{n}{2}} = \frac{2}{n}$, mentre il costo 
    di entrambi gli insimemi al punto 1, $\frac{1+\epsilon}{\frac{n}{2}} = \frac{2+2\epsilon}{n}$

    Al secondo passo, si preferisce ai primi due l'insieme al punto 3, il calcolo è simile al punto precedente.\\
    \dots

    Il costo che si ottiene è $w = \log n$
    La soluzione ottima sarebbe quella di prendere al passo 1 i primi due insiemi, in modo da coprire l'intero 
    universo, ovvero $w^* = 2 + 2\epsilon$.
\end{proof}

\subsubsection{Vertex Cover}
Il problema di Vertex Cover consiste nel trovare una copertura di 
vertici in un grafo tale per cui per ogni vertice, una delle due estremità è contenuta nella copertura.

Formalmente: \\
\emph{Input}: $G=(V,E), w_i \in \mathbb{Q}^{>0}, \forall i \in V$\\
\emph{Output}: $X \subseteq V, \forall xy \in E, x \in X \vee y \in X$\\
\emph{Costo}: $w = \sum_{i \in X} w_i$\\
\emph{Tipo}: min\\

Consideriamo la versione di decisione del problema, ovvero $\hat{\mathit{VertexCover}}$, cioè, 
posso trovare una copertura che ha peso minore di $\theta$?\\

Vale che $\hat{\mathit{VertexCover}} \leq_{p} \hat{\mathit{SetCover}}$
Per effettuare il passaggio, bisogna passare dall'input del primo al secondo
$$G=(V,E), w_i \in \mathbb{Q}^{>0}, \theta \longrightarrow f \longrightarrow S_1, \dots, S_m, \bar{W_1}, \dots, \bar{W_m},\bar{\theta}$$
La funzione $f$ funziona così: 
\begin{equation}
    \begin{aligned}
        S_i = \{ e \in E, i \in e \} && \text{Per ogni vertice considero i suoi vicini}\\
        U = E, \bar{W_i} = W_i, \bar{\theta} = \theta
    \end{aligned}
\end{equation}
\begin{remark}
    La funzione $f$ può essere utilizzata per mappare anche VertexCover in SetCover di ottimizzazione, 
    visto che non stravolge il problema.\\
    La trasformazione appena discussa quindi può essere utilizzata per fornire una $\log n$-approssimazione
    per VertexCover di ottimizzazione.
\end{remark}
\paragraph{Price Vertex Cover}
Prima di definire l'algoritmo vero e proprio, sono necessari alcuni passaggi preliminari.
L'idea si basa sul fatto che gli archi sono intenzionati a comprare uno dei due estremi,
ad un certo prezzo.
Un nodo si vende, se la somma delle offerte degli archi incidenti arriva al suo $W_i$.
\begin{definition}
    Un insieme di offerte si dice equo per un vertice sse:
    $$\sum_{e\in E, i \in e}P_e \leq W_i$$
    Ovvero se le offerte $P_e$ degli archi che incidono sul vertice $i$ 
    non superano il suo costo, ovviamente devono raggiungerlo per comprare 
    il vertice.    
\end{definition}
\begin{lemma}
    \label{pscl1}
    Se $P_e$ è equo, allora
    $$\sum_{e \in E} P_e \leq w^*$$
\end{lemma}
\begin{proof}
    La definizione di equità implica che
    $$\forall i \in V \sum_{e\in E, i \in e}P_e \leq W_i$$
    Consideriamo ora la somma delle disequazioni per la soluzione ottima
\begin{equation}
    \begin{aligned}
        \sum_{i \in S^*} \sum_{e\in E, i \in e}P_e \leq \sum_{i \in S^*}W_i\\
        \sum_{e \in E} P_e \leq \sum_{i \in S^*} \sum_{e\in E, i \in e}P_e \leq W^*
    \end{aligned}
\end{equation}
Questo vale perchè la seconda parte della disequazione considera potenzialmente più volte alcuni lati.
\end{proof}
\begin{definition}
    $P_e$ è stretto sul vertice $i$ sse
    $$\sum_{e \in E, i \in e} P_e = W_i$$
\end{definition}
Ecco ora l'algoritmo.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwIn{$G=(V,E), W_i \forall i \in V$}
    \KwResult{Cover per il grafo}
     $P_e \gets 0, \forall e \in E$\\
     \While{$\exists ij \in E, P_e$ non stretto su i o su j}{
        $\bar{e} \gets \bar{i}\bar{j}$\\
        $\Delta \gets \min(W_{\bar{i}}-\sum_{e \in E, \bar{i} \in e}P_e, W_{\bar{j}}-\sum_{e \in E, \bar{j} \in e}P_e,)$\\
        $P_e \gets P_e + \Delta$
     }
     \Return{$\{ i| P_e$ stretto su $i\}$}
     \caption{PriceVertexCover}
\end{algorithm}
L'idea su cui si basa l'algoritmo è quella che, se un arco non sta offrendo abbastanza per i vertici, allora 
aumenta la sua offerta, del minimo per soddisfare uno dei due nodi.
\begin{lemma}
    \label{pscl2}
    Price Set Cover crea un peso $$W \leq 2\sum_{e \in E}P_e$$
\end{lemma}
\begin{proof}
    Il costo finale di PSC è $W = \sum_{i \in S}W_i$, ovvero il costo 
    dei vertici in output. Vale che, per la definizione di strettezza:
    $$W = \sum_{i \in S}W_i = \sum_{i \in S^*} \sum_{e\in E, i \in e}P_e$$
    Nella seconda sommatoria un lato compare una o due volte, se rispettivamente 
    una o due estremità appartengono ad $S$.
\end{proof}
\begin{theorem}
    Price Set Cover è $2$-approssimante per Set Cover.
\end{theorem}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            \frac{w}{w^*} \leq \frac{2\sum_{e \in E}P_e}{w^*} && \text{Per il lemma \ref{pscl2}}\\
            \frac{w}{w^*} \leq \frac{2\sum_{e \in E}P_e}{\sum_{e \in E} P_e } \leq 2 && \text{Per il lemma \ref{pscl1}}
        \end{aligned}
    \end{equation}
\end{proof}
\begin{remark}
    Non esistono ad oggi algoritmi migliori per Price Set Cover.
\end{remark}

\subsubsection{Disjoint Paths}
Dato un grafo orientato, esistono $K$ sorgenti e target, l'obiettivo è trovare $K$ cammini per le 
coppie di sorgenti e destinazioni. Ogni arco può essere usato al più $C$ volte.

Formalmente: \\
\emph{Input}: $G=(V,E)$ orientato, $s_1, \dots, s_k$ sorgenti, $t_1, \dots, t_k$ destinazioni $\in V$
$C \in \mathbb{N}^{>0}$\\
\emph{Output}: $I = \{1, \dots, K\}, \forall i \in I,$ cammino $\Pi_i$, tale che nessun arco è utilizzato più di $C$ volte\\
\emph{Funzione obiettivo}: $|I|$\\
\emph{Tipo}: max\\

L'idea su cui si basa l'algoritmo è quella di allungare un arco tanto quanto è utilizzato, 
in modo da scoraggiare il suo utilizzo.\\
È quindi definita una funzione di lunghezza: 
$$l : E \longrightarrow \mathbb{R}^{>0}$$
Dato un cammino come sequenza di nodi, la lunghezza del cammino sarà data dalla lunghezza su ogni coppia di consecutivi.

\paragraph{Greedy Paths with capacity}
L'algoritmo considera come input, oltre a quello per Disjoint Paths, un parametro
$\beta > 0$, che indica il fattore di allungamento degli archi.

\begin{algorithm}[H]
    \SetAlgoLined
    \KwIn{$G=(V,E),s_1, \dots, s_k, t_1, \dots, t_k, \beta > 0$}
    \KwResult{Cammini da sorgenti a destinazioni}
     $I \gets \emptyset$\\
     $P \gets 0$\\
     $l(xy) \gets 1, \forall xy \in E$\\
     \While{$\Pi \gets$ shortest path from $s_i$ to $t_i$, $i \notin I$}{
         $I \gets I \cup i$\\
         $P \gets P \cup \Pi$\\
         \For{$e\in \Pi$}{
            $l(e) \gets l(e)*\beta$
            \If{$e$ used by C paths}{
                $\mathit{remove}(e)$
            }
         }
     }
     \Return{$I,P$}
     \caption{GreedyDisjointPaths}
\end{algorithm}

Consideriamo ora l'evolversi dei cammini nell'algoritmo, in particolare un cammino
può essere:
\begin{itemize}
    \item Inutile, se unisce una sorgente e destinazione già collegate
    \item Utile ma ostruito, ovvero collega una sorgente e destinazione sconnesse, 
    ma passa per un arco utlizzato da più di $C$ cammini
\end{itemize}

\begin{definition}
    Un cammino $\Pi$ è corto rispetto a una certa lunghezza $l$ sse
    $l(\Pi) < \beta^c$, lungo altrimenti.
\end{definition}
\begin{remark}
    \label{ossdisjoint}
    Se sono presenti cammini utili non ostruiti e corti, l'algoritmo sceglie un cammino
    di questa natura, inoltre, nelle prime fasi dell'algoritmo, ovvero quando sono presenti cammini
    di questo tipo, si può evitare di eliminare archi.
\end{remark}
\begin{proof}
    Se esite un cammino corto, la sua lunghezza è per forza minore di $\beta^c$ quindi nessun arco 
    è stato utilizzato $C$ volte.
\end{proof}

Tenendo presente quanto detto nell'osservazione \ref{ossdisjoint}, teniamo ora in considerazione 
la funzione di lunghezza $\bar{l}$ nell'esatto istante in cui finiscono i cammini corti.
Inoltre, $\bar{I}, \bar{P}$ saranno le sorgenti, destinazioni collegate e i rispettivi cammini 
in quell'istante.

\begin{lemma}
    \label{ldis1}
    Se $i \in I^* \setminus I$, allora $\bar{l}(\Pi_i^*) \geq \beta^c$
\end{lemma}
\begin{proof}
    Se valesse $\bar{l}(\Pi_i^*) < \beta^c$, il cammino $\Pi_i^*$ sarebbe corto e
    utile, in quanto collega una coppia ancora non collegata, dato che non appartiene a $I$.
    Quindi, non esisterebbe motivo per non selezionarlo.
\end{proof}

\begin{lemma}
    \label{ldis2}
    Vale che $\sum_{e \in E}\bar{l}(e) \leq \beta^{c+1}|\bar{I}| + m$.
\end{lemma}
\begin{proof}
    All'inizio dell'algoritmo, $\sum_{e \in E} l(e) = m$.
    Consideriamo ora le due funzioni di lunghezza dopo la selezione di un cammino
    $$l_1 \longrightarrow \Pi \longrightarrow l_2$$
    \[ 
        l_2(e) = 
        \begin{cases}
        \beta l_1(e) & \mathit{se}\;e \in \Pi\\
        l_1(e) & \mathit{se}\;e \notin \Pi
     \end{cases}
    \]
    Consideriamo ora la differenza delle lunghezze
    \begin{equation}
        \begin{aligned}
            \sum_{e \in E} l_1(e) - \sum_{e \in E} l_2(e) = \sum_{e \in E} (l_1(e) - l_2(e))\\
            = \sum_{e \in E} (l_1(e) - l_2(e)) = \sum_{e \in \Pi} (\beta l_1(e) - l_2(e))\\
            = \sum_{e \in \Pi} (\beta -1)l_1(e) = (\beta -1) \sum_{e \in \Pi} l_1(e)\\
            = (\beta -1)l(\Pi) \leq (\beta -1)\beta^c \leq \beta^{c+1}
        \end{aligned}
    \end{equation}
    Considerando l'inizio della dimostrazione, visto che ad ogni passo la lunghezza aumenta di al 
    massimo $\beta^{c+1}$, il lemma vale.
\end{proof}
\begin{remark}
    \label{oss3dis}
    Valgono:
    \begin{equation}
        \begin{aligned}
            \sum_{i \in I^* \setminus I} \bar{l}(\Pi_i^*) \geq \beta^c|I^* \setminus I| && \text{dal lemma \ref{ldis1}}\\
            \sum_{i \in I^* \setminus I} \bar{l}(\Pi_i^*) \leq c\sum_{e \in E} \bar{l}(e) \leq c(\beta ^{c+1}|\bar{I} + m|)
            && \text{per il lemma \ref{ldis2}}
        \end{aligned}
    \end{equation}
\end{remark}

Mettendo tutto insieme: 
\begin{equation}
    \begin{aligned}
        \beta^c|I^*| = \beta^c|I^* \setminus I| + \beta^c|I^* \cap I|\\
        \leq \sum_{i \in I^* \setminus I} \bar{\Pi_i^*} + \beta^c && \text{per il punto 1 di oss. \ref{oss3dis}, e maggioro}\\
        \leq c(\beta^{c+1}|\bar{I}| + m) + \beta^c |I|&& \text{per il punto 2 di oss. \ref{oss3dis}}\\
        \leq c(\beta^{c+1}|I| + m) + \beta^c |I|&& \text{I è sovrainsieme per I barra}\\
        |I^*| \leq c\beta|I| + c\beta^{-c}m + |I|&& \text{divido per beta elevato alla c}\\
        |I^*| \leq c\beta|I| + c\beta^{-c }m|I| + |I|&& \text{moltiplico per la cardinalità quello in mezzo}\\
        \frac{|I^*|}{|I|} \leq c\beta + c\beta^{-c}m + 1&& \text{divido per cardinalità di I}
    \end{aligned}
\end{equation}
Poniamo ora $\beta = m^{\frac{1}{c+1}}$ si ottiene:
\begin{equation}
    \begin{aligned}
        \frac{|I^*|}{|I|} \leq c(m^{\frac{1}{c+1}} + m^{\frac{-c+c+1}{c+1}}) + 1\\
        \frac{|I^*|}{|I|} \leq 2cm^{\frac{1}{c+1}} +1
    \end{aligned}
\end{equation}

Al variare di $C$ varia il grado di approssimazione dell'algoritmo.
\begin{center}
    \begin{tabular}{ c c  }
     C & Bound approssimazione \\
     \hline 
     1 & $2\sqrt[2]{m} + 1$ \\  
     2 & $4\sqrt[3]{m} + 1$ \\  
     3 & $6\sqrt[4]{m} + 1$ \\  
    \dots & \dots
    \end{tabular}
\end{center}
\begin{theorem}
    Greedy Disjoint Paths con  $\beta = m^{\frac{1}{c+1}}$ fornisce una $(2cm^{\frac{1}{c+1}} +1)$-approssimazione.
\end{theorem}
\begin{remark}
    L'algoritmo non è buono, il fattore di approssimazione è pessimo, infatti, funziona decentemente solo 
    se il numero di coppie da collegare $K >> 2\sqrt[2]{m}$.\\
    Si fa notare infine che l'algoritmo funziona anche se una coppia è da collegare più volte.
\end{remark}

\subsection{Tecniche di arrotondamento}

\subsubsection{Linear programming}
Un problema di programmazione lineare è formato da una funzione obiettivo e dei vincoli.
\paragraph{Esempio}
Funzione obiettivo: $$\mathit{min}\;\;3x_1 + |7x_2 - 4x_3|$$
Vincoli:
\[ 
    \begin{cases}
        x_1 + x_2 \leq 3\\
        3x_3 - x1 \geq 7\\
        x1 \geq 0\\
        x2 \geq 0\\
        x3 \geq 0\\
    \end{cases}
\]

Più formalmente:\\
\emph{Input}: $A \in \mathbb{Q}^{m\times n}, b \in \mathbb{Q}^m, c \in \mathbb{Q}^n$\\
\emph{Output}: $x \in \mathbb{R}^n, Ax \geq b$\\
\emph{Funzione obiettivo}: $C^T \cdot x$, valore della funzione obiettivo\\
\emph{Tipo}: max, min\\

\begin{remark}
    Il problema di  LP $\in $ PO, spesso però si utilizzano
    algoritmi worst case esponenziali.
\end{remark}

\paragraph{Integer linear programming}
Cambiando il vincolo della soluzione ai soli interi, il problema diventa NPO completo.

Ovvero:\\
\emph{Input}: $A \in \mathbb{Q}^{m\times n}, b \in \mathbb{Q}^m, c \in \mathbb{Q}^n$\\
\emph{Output}: $x \in \mathbb{Z}^n, Ax \geq b$\\
\emph{Funzione obiettivo}: $C^T \cdot x$, valore della funzione obiettivo\\
\emph{Tipo}: max, min\\

\begin{theorem}
    $\hat{\mathit{VertexCover}} \leq_p \hat{\mathit{ILP}}$ 
\end{theorem}
\begin{proof}
    Consideriamo un input per il problema di $\hat{\mathit{VertexCover}}$, ovvero $$(G=(E,V), w_i \forall i \in V, \theta)$$
    Ci si chiede se: $$\exists X \subseteq V | \forall xy \in E, x \in X \mathit{or} y \in X, \sum_{i\in X} w_i \leq \theta$$
    Costruisco ora il problema di programmazione linerare in questo modo:
    $$x_i \forall i \in V$$
    Vincoli:
    \[
        2(n+m)\;\mathit{volte}
        \begin{cases}
            x_i \geq 0 \;\forall i \in V\\
            x_i \leq 1 \;\forall i \in V\\
            x_i + x_j \geq 1 \;\forall ij \in E\\
        \end{cases}
    \]
    Obiettivo,:
    $$\min \sum_{i\in V} w_ix_i$$

    Assumiamo ora $\bar{x_i}$ soluzione ammissibile per ILP. Sia $X = \{ i | \bar{x_i} = 1\}$.
    Visto che c'è il vincolo sugli archi, $X$ è soluzione per $\hat{\mathit{VertexCover}}$.

    Quindi, risolvendo in tempo polinomiale ILP si risolverebbe VertexCover, ma 
    visto che $\hat{\mathit{VertexCover}}$ è NP completo, anche ILP è NP completo.
\end{proof}

\subsubsection{Vertex Cover con arrotondamento}
Consideriamo ora la trasformazione mostrata al teorema precedente, ma invece
che un problema di programmazione lineare intera consideriamone uno reale, $\Pi^\prime$.

Esiste il problema di fare il passo inverso. Con il problema intero infatti 
bastava considerare le variabili con valore 1, ora sono reali, bisogna quindi trovare un 
modo di capire se una variabile (nodo), fa o no parte del cover.

Si sta di fatto rilassando il problema, chiamiamo $\Pi^\prime_{\mathit{INT}}$ il problema 
intero, rispetto a quello reale si avrà che:\footnote{Si sta considerando un problema di minimo}
\begin{equation}
    \begin{aligned}
        x_i^*, w^*, \tilde{x_i^*}, \tilde{w^*} && \text{Soluzioni rispettivamente reale e intera}\\
        \tilde{w^*} \geq w^* && \text{Poichè il dominio di ricerca reale è maggiore}\\
    \end{aligned}
\end{equation}

Si definisce ora la seguente trasformazione:
\[
    r_i = 
    \begin{cases}
        1\;\mathit{se}\;x_i^* \geq \frac{1}{2}\\
        0\;\mathit{se}\;x_i^* < \frac{1}{2}
    \end{cases}
\]
\begin{remark}
    La trasformazione fornisce una soluzione ammissibile per $\Pi^\prime_{\mathit{INT}}$.
\end{remark}
\begin{proof}
    Bisogna dimostrare che:
    $$\forall ij \in E, r_i + r_j \geq 1$$
    Sapendo che:
    $$\forall ij \in E, x_i^* + x_j^* \geq 1$$
    Supponiamo per assurdo che valgano entrambi 0, ma allora: 
    $$x_i^* < \frac{1}{2}, x_j^* < \frac{1}{2}$$
    che contraddice la seconda assunzione.
\end{proof}
\begin{theorem}
    Arrotondando la soluzione reale con la trasformazione mostrata, 
    si fornisce una 2-approssimazione per $\hat{\mathit{VertexCover}}$.
\end{theorem}
\begin{proof}
    Valgono:
    \begin{equation}
        \begin{aligned}
            r_i \leq 2x_i^* && \text{Per definizione della trasformazione}\\
            \sum_{i\in V} w_ir_i \leq 2 \sum_{i\in V} w_ix_i^* = 2w^* \leq 2\tilde{w} &&\text{La soluzione arrotondata è al più il doppio}
        \end{aligned}
    \end{equation}
\end{proof}